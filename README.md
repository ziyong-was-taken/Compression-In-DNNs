# TOC

- [TOC](#toc)

## Networks

- supported models
  - MLP with custom widths and non-linearity
  - ConvNeXt-T
  - ResNet-18
- access activations $\{ğ¡Ë¡_{c,i}\}_{lâˆˆâ„’}$ using `model.feature_extractor(`$ğ±_{c,i}$`)`
  - MLP: end of each non-linearity
  - ConvNeXt-T, ResNet-18: end of each residual block
- `model.get_encoder_decoder(`$l$`)` returns encoder up to layer $l$ and decoder starting from layer $l$
- compute metrics after each epoch using `on_train_epoch_end` callback of trainer (to access data)

## Metrics

### NC1

- goal: compute $\operatorname{tr}(Î£_W^l Î£_B^{l+})$ for all layers $l âˆˆ â„’$
- issue: only batch activations $\{ğ¡Ë¡_{c,i}\}_{l âˆˆ â„’,\ c âˆˆ \{1,â€¦,C\},\ i âˆˆ \{1,â€¦,S\}}$ fit in memory (batch size $S$)
- when training a batch, store running
  - class counts $\{n_c\}_{c=1}^C$
  - layer class sums $\{\{âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l\}_{c=1}^C\}_{l âˆˆ â„’}$
  - gram matrix $G^l = âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l ğ¡_{c,i}^{lâŠ¤}$
- $Î£_B^l$
  - use class counts and layer sums to compute $\{\boldsymbol Î¼_c^l\}_{l âˆˆ â„’}$
  - sum class counts and sums to obtain $\{\bar{\boldsymbol Î¼}^l\}_{l âˆˆ â„’}$
  - $Î£_B^l = 1/C âˆ‘_{c=1}^C ({\boldsymbol Î¼}_c^l - \bar{\boldsymbol Î¼}^l)({\boldsymbol Î¼}_c^l - \bar{\boldsymbol Î¼}^l)^âŠ¤$
- $Î£_W^l$
  - recall: $Î£_W^l + Î£_B^l = Î£_T^l = 1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c}(ğ¡_{c,i}^l - \bar{\boldsymbol Î¼}^l)(ğ¡_{c,i}^l - \bar{\boldsymbol Î¼}^l)^âŠ¤ = Î£_W^l + Î£_B^l$
  - lemma: $Î£_T^l = G^l/N - \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤}$
    $$
    \begin{align*}
    Î£_T^l
      &= 1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c}(ğ¡_{c,i}^l - \bar{\boldsymbol Î¼}^l)(ğ¡_{c,i}^l - \bar{\boldsymbol Î¼}^l)^âŠ¤ \\
      &= 1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} (ğ¡_{c,i}^l ğ¡_{c,i}^{lâŠ¤} - \bar{\boldsymbol Î¼}^l ğ¡_{c,i}^{lâŠ¤} - ğ¡_{c,i}^l \bar{\boldsymbol Î¼}^{lâŠ¤} + \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤}) \\
      &= 1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l ğ¡_{c,i}^{lâŠ¤} - 1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} (\bar{\boldsymbol Î¼}^l ğ¡_{c,i}^{lâŠ¤} + ğ¡_{c,i}^l \bar{\boldsymbol Î¼}^{lâŠ¤}) + \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} \\
      &= G^l/N - \bar{\boldsymbol Î¼}^l\left(1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^{lâŠ¤}\right) - \left(1/N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l\right) \bar{\boldsymbol Î¼}^{lâŠ¤} + \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} \\
      &= G^l/N - \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} - \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} + \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} \\
      &= G^l/N - \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤}
    \end{align*}
    $$
  - thus, $Î£_W^l = G^l/N - \bar{\boldsymbol Î¼}^l \bar{\boldsymbol Î¼}^{lâŠ¤} - Î£_B^l$

### DIB

- compute new labels for all samples using Algorithm 1
- for each new label, create a copy of the decoder $D$ returned by `model.get_encoder_decoder()`
- combine encoder $E$ and decoders $D_1, D_2, \dots$ into single model $M$
  <!---->
  ```markdown
         E
  M =  / | \
      Dâ‚ Dâ‚‚ â€¦
  ```
  <!---->
- after each training epoch, train $M$ using average cross-entropy loss over the decoder heads

### Rank
