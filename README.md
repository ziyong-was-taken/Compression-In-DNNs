# Explaining Deep Neural Networks using Information Theory and Geometry

This repository contains the code to reproduce the results of my master's thesis.
It is mainly implemented using [(PyTorch) Lightning](https://lightning.ai/docs/pytorch/stable/).

<!-- omit in toc -->
## Table of Contents

- [Usage](#usage)
  - [Requirements](#requirements)
  - [Flags](#flags)
  - [Plotting Results](#plotting-results)
- [Code Structure](#code-structure)
- [Algorithms](#algorithms)
  - [Modified Algorithm 1](#modified-algorithm-1)
  - [NC Computation](#nc-computation)
  - [DIB Computation (for layer $l$)](#dib-computation-for-layer-l)

## Usage

The easiest way to run the code is to use [conda](https://conda.org/).
Simply create a new environment using the provided [`environment.yaml`](./environment.yaml) file and the following commands:

```bash
# create and activate the environment (name: master_thesis)
conda env create -f environment.yaml
conda activate master_thesis

# run the code (possibly with flags)
python main.py --flag value

# (optional) deactivate the environment afterwards
conda deactivate
```

The repository also includes a minimal [micromamba-docker](https://micromamba-docker.readthedocs.io/en/latest/index.html) Dockerfile.
Build and run the [Docker](https://www.docker.com/) image by using the following commands:

```bash
# build the Docker image (tag: master_thesis)
docker build -t master_thesis .

# run the Docker container
docker run -it master_thesis python main.py --flag value
              ^
              insert --gpus all here to use GPUs
```

There is also an [Apptainer](https://apptainer.org/) definition file `master_thesis.def` which can be used like so:

```bash
# build the Apptainer container (filename: master_thesis.sif)
apptainer build master_thesis.sif master_thesis.def

# run the Apptainer container
./master_thesis.sif python main.py --flag value

# run the Apptainer container (with GPUs)
apptainer run --nv master_thesis.sif python main.py --flag value
```

### Requirements

- `lightning>=2.5.1`: [(PyTorch) Lightning](https://lightning.ai/docs/pytorch/stable/) is the main framework used
- `torchvision>=0.21.0`[^CUDA]: datasets and models are imported from [torchvision](https://pytorch.org/vision/stable/index.html)
- `cuda_compiler>=12.8.1` CUDA compiler for [`torch.compile()`](https://pytorch.org/docs/stable/generated/torch.compile.html) (also installs C and C++ compilers)
- `ipykernel>=6.29.5`: necessary to run Jupyter notebook
- `matplotlib>=3.10.1`: used to create plots
- `pandas>=2.2.3`: used to read csv data

[^CUDA]: The conda solver prioritises the CPU build of `torchvision` (over the CUDA build) due to its higher build number.
To force the CUDA build, use `torchvision>=0.21.0=cuda*` instead.

### Flags

- Example flags include `-m` to choose the model and `-d` to choose the dataset.
- For example,
  <!---->
  ```bash
  python main.py -m MLP -d MNIST
  ```
  <!---->
  uses a multi-layer perceptron model and the MNIST dataset.
- supported models (case-sensitive):
  - `MLP`: multi-layer perceptron with custom widths specified by `-w` and nonlinearity specified by `-nl`
  - `MNISTNet`: simplified `CIFARNet`
  - `CIFARNet`: based on [CIFAR-10 Airbench architecture](https://github.com/KellerJordan/cifar10-airbench) from [94% on CIFAR-10 in 3.29 Seconds on a Single GPU](https://arxiv.org/abs/2404.00498)
  - `ConvNeXt`: the [ConvNeXt-T architecture](https://pytorch.org/vision/0.21/models/generated/torchvision.models.convnext_tiny.html) from [A ConvNet for the 2020s](https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html)
  - `ResNet`: the [ResNet-18 architecture](https://pytorch.org/vision/0.21/models/generated/torchvision.models.resnet18.html) from [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)
- supported datasets (case-sensitive):
  - `CIFAR10`: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
  - `FashionMNIST`: [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)
  - `MNIST`: [MNIST](http://yann.lecun.com/exdb/mnist/)
  - `SZT`: the dataset (included in the repository) used by [Schwartz-Ziv & Tishby (2017)](https://arxiv.org/abs/1703.00810)
- For a full list of flags, run `python main.py --help`.
- Further information is also available in the `get_args()` function of [`utils.py`](./utils.py).

### Plotting Results

To plot the results, run the cells in the Jupyter notebook [`plots.ipynb`](./plots.ipynb) using the conda environment from above as the kernel.
Alternatively, any kernel with `ipykernel>=6.29.5`, `matplotlib>=3.10.1`, and `pandas>=2.2.3` installed should also work.

## Code Structure

The code consists of four main Python modules and one Jupyter notebook:

- [`datasets.py`](./datasets.py): contains logic for loading and transforming the datasets; also contains a slightly modified version of Algorithm 1 of [Learning Optimal Representations with the Decodable Information Bottleneck](https://proceedings.neurips.cc/paper_files/paper/2020/hash/d8ea5f53c1b1eb087ac2e356253395d8-Abstract.html) (see [Modified Algorithm 1](#modified-algorithm-1))
- [`main.py`](./main.py): "Glue code" which
  - sets up the dataset(s),
  - uses an (optional) learning rate (LR) tuner based on the LR range test of [Cyclical Learning Rates for Training Neural Networks](https://ieeexplore.ieee.org/document/7926641), and
  - creates and trains the model.
- [`networks.py`](./networks.py): network architectures
- [`utils.py`](./utils.py): contains the command line flag parser and the algorithms for computing the NC metric and the DIB (see [Algorithms](#algorithms) for more details)
- [`plots.ipynb`](./plots.ipynb): notebook for creating plots

When using a dataset for the first time, Lightning will download it into the directory specified by `--data-dir` (default: `data/`).
Lightning stores model checkpoints for the main network in `lightning_logs/version_X/checkpoints/` and model checkpoints for the DIB network in `lightning_logs/checkpoints`.
Hyperparameters are stored in `lightning_logs/version_X/hparams.yaml` and the metrics are stored in `lightning_logs/version_X/metrics.csv`.

## Algorithms

Both the NC and DIB metrics are computed after each epoch for both the train and test set.

### Modified Algorithm 1

1. For each class $c$, enumerate the samples $\mathcal X_c = \{x âˆ£ x$ has label $y = c\}$, i.e., assign them the indices $0,1,â€¦,n_c - 1$ (recall that $n_c = |\mathcal X_c|$)
2. Convert each index to base $C$ (the number of classes), implicitly padding with zeros to the left. It is easy to see that the maximum number of digits is $N_D = âŒˆ\log_C(\max_c n_c)âŒ‰$.
3. The new labels for each sample are the digits of its base $C$ representation.

<!-- markdownlint-disable MD033 -->
<details>
<summary>Example</summary>

Let $C = 5$ and the samples be labeled [0, 0, 0, 1, 1, 3, 4, 4, 4, 4, 4, 4].
Then, the samples are assigned the indices [0, 1, 2, 0, 1, 0, 0, 1, 2, 3, 4, 5].
Converting to base $C = 5$ and zero-padding to the left gives \[[0, 0], [0, 1], [0, 2], [0, 0], [0, 1], [0, 0], [0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [1, 0]\].
The new labels are then [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] and [0, 1, 2, 0, 1, 0, 0, 1, 2, 3, 4, 0].

</details>
<!-- markdownlint-enable MD033 -->

### NC Computation

- Goal: each epoch, compute $\operatorname{tr}(ğšº_W^l (ğšº_B^l)âº)$ for all layers $l âˆˆ \{1,â€¦,L\}$
- The activations $\{ğ¡Ë¡_{c,i}\}_{lâˆˆ\{1,â€¦,L\},\ c âˆˆ \{1,â€¦,C\},\ i âˆˆ \{1,â€¦,N\}}$ are accessed by registering forward hooks at the penultimate layer as well as:
  - MLP: after each nonlinearity
  - MNISTNet: after each convolutional block
  - CIFARNet: after each downsampling layer
  - ConvNeXt-T, ResNet-18: after each residual block
  - these hooks store the output of each hooked layer after each forward pass
- Since the activations don't fit in memory all at once, only the batch activations $\{ğ¡Ë¡_{c,i}\}_{l âˆˆ \{1,â€¦,L\},\ c âˆˆ \{1,â€¦,C\},\ i âˆˆ \{b_1,â€¦,b_S\}}$ can be used where $b_n$ is element $n$ of batch $b$ and $S$ is the batch size.
- Furthermore, for large CNNs, $ğ¡Ë¡_{c,i} \in â„^{Dâ‰¤10âµ}$, which makes computing $ğšº_W^l âˆˆ â„^{DÃ—D}$ and $ğšº_B^l âˆˆ â„^{DÃ—D}$ directly undesirable.

<!-- omit in toc -->
#### The Algorithm (for layer $l$)

The algorithm requires *two* passes over the dataset.

0. Before training, compute the class counts $\{n_c\}_{c=1}^C$.
   This is possible since the labels $(âˆˆâ„•^{Nâ‰ˆ10âµ})$ fit in memory.
1. (**first pass**): After each batch, update the running class totals $\{\{âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l\}_{c=1}^C\}_{l =1}^L$.
2. Using the final class totals,
   - compute $\boldsymbol Î¼_c^l = \frac 1{n_c} âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l$ for $c = 1,â€¦,C$
   - compute $\bar{\boldsymbol Î¼}Ë¡ = \frac 1N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} ğ¡_{c,i}^l$ where $N = ğšº_{c=1}^C n_c$
   - compute $ğŒË¡ = [\boldsymbol Î¼â‚Ë¡ - \bar{\boldsymbol Î¼}Ë¡, â‹¯, \boldsymbol Î¼_C^l - \bar{\boldsymbol Î¼}Ë¡]$, recall: $ğšº_B^l = \frac1C ğŒË¡(ğŒË¡)^âŠ¤$
   - since $ğŒË¡(ğŒË¡)^âŠ¤$ and $(ğŒË¡)^âŠ¤ğŒË¡$ share eigenvalues (see proof of step 4), compute the eigendecomposition of (the much smaller matrix) $(ğŒË¡)^âŠ¤ğŒË¡ = ğ•ğš²ğ•^âŠ¤$
3. (**second pass**): After each batch, update the running sum $S_{\operatorname{tr}} = âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} âˆ‘_{j=1}^r \left(\frac{(Îº_{c,i})_j}{Î»_j}\right)Â²$ where $Îº_{c,i} = ğ•^âŠ¤(ğŒË¡)^âŠ¤(ğ¡_{c,i}^l - \boldsymbol Î¼_c^l) âˆˆ â„^C$ and $ğš² = \operatorname{diag}(Î»â‚,â€¦,Î»_r,0,\dots,0) âˆˆ â„^{CÃ—C}$.
4. Finally, $\operatorname{tr}(ğšº_W^l(ğšº_B^l)âº) = \frac CN S_{\operatorname{tr}}$

<!-- markdownlint-disable MD033 -->
<details>
<summary>Proof of Step 4</summary>

Consider the SVD $ğŒË¡ = ğ”ğ’ğ•^âŠ¤$. We then have $ğŒË¡ğ•ğ’âº = ğ”$ and
$$
  ğŒË¡(ğŒË¡)^âŠ¤ = ğ”ğ’ğ•^âŠ¤ğ•ğ’ğ”^âŠ¤ = ğ”ğ’Â²ğ”^âŠ¤ = ğ”ğš²ğ”^âŠ¤ \\
  (ğŒË¡)^âŠ¤ğŒË¡ = ğ•ğ’ğ”^âŠ¤ğ”ğ’ğ•^âŠ¤ = ğ•ğ’Â²ğ•^âŠ¤ = ğ•ğš²ğ•^âŠ¤
$$
By definition of the pseudoinverse, since $ğšº_B^l = \frac1C ğŒË¡(ğŒË¡)^âŠ¤ = ğ”(ğš²/C)ğ”^âŠ¤$,
$$
  (ğšº_B^l)âº = ğ”(ğš²/C)âºğ”^âŠ¤ = Cğ”ğš²âºğ”^âŠ¤ = CğŒË¡ğ•ğ’âºğš²âºğ’âºğ•^âŠ¤(ğŒË¡)^âŠ¤ = CğŒË¡ğ•(ğš²Â²)âºğ•^âŠ¤(ğŒË¡)^âŠ¤.
$$
Now, since $\operatorname{tr}(ğšğ›^âŠ¤) = ğ›^âŠ¤ğš$ for vectors $ğš,ğ›$, we have
$$
\begin{align*}
  \operatorname{tr}(ğšº_W^l (ğšº_B^l)âº)
    &= \frac1N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} \operatorname{tr}((ğ¡_{c,i}^l - \boldsymbol Î¼_c^l)(ğ¡_{c,i}^l - \boldsymbol Î¼_c^l)^âŠ¤(ğšº_B^l)âº) \\
    &= \frac1N âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} (ğ¡_{c,i}^l - \boldsymbol Î¼_c^l)^âŠ¤(ğšº_B^l)âº(ğ¡_{c,i}^l - \boldsymbol Î¼_c^l) \\
    &= \frac CN âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} (ğ¡_{c,i}^l - \boldsymbol Î¼_c^l)^âŠ¤ğŒË¡ğ•(ğš²Â²)âº\underbrace{ğ•^âŠ¤(ğŒË¡)^âŠ¤(ğ¡_{c,i}^l - \boldsymbol Î¼_c^l)}_{Îº_{c,i}} \\
    &= \frac CN âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} Îº_{c,i}^âŠ¤
        \left[\begin{array}{ccc|c}
          \frac1{\lambdaâ‚Â²} &        & ğŸ                   &   \\
                            & \ddots &                     & ğŸ \\
          ğŸ                 &        & \frac 1{\lambda_rÂ²} &   \\
          \hline
                            & ğŸ      &                     & ğŸ \\
        \end{array}\right]Îº_{c,i} \\
    &= \frac CN âˆ‘_{c=1}^C âˆ‘_{i=1}^{n_c} âˆ‘_{j=1}^r \left(\frac{(Îº_{c,i})_j}{Î»_j}\right)Â².
\end{align*}
$$

</details>
<!-- markdownlint-enable MD033 -->

### DIB Computation (for layer $l$)

1. Compute $N_D$ new labels $\{\mathcal Y^{DIB}_{N_d}\}_{N_d=1}^{N_D}$ for all samples using [modified Algorithm 1](#modified-algorithm-1).
2. Split the original network $f^{L:1}$ into an encoder $f^{l:1}$ and decoder $f^{L:l+1}$ for some layer $l$.
3. Freeze $f^{l:1}$.
4. Combine $f^{l:1}$ and $N_D$ copies of $f^{L:l+1}$ into a single model $M$ which looks like
   $$
    \begin{matrix}
              &   & f^{L:l+1}_{N_1} \\
              & â•± &             \\
      f^{l:1} & â€” & \vdots      \\
              & â•² &             \\
              &   & f^{L:l+1}_{N_D} \\
    \end{matrix}
   $$
5. Train $M$ on $\mathcal Y^{DIB}$ and $f^{L:1}$ on the original labels $\mathcal Y$ using cross entropy loss.
6. The DIB terms are then
   <!---->
   - sufficiency: $H(Y) - â„“_{CE}(f^{L:1}, \mathcal Y)$
   - minimality: $\frac 1{N_D} âˆ‘_{N_d=1}^{N_D} H(\mathcal Y^{DIB}_{N_d}) - â„“_{CE}(M_{N_d}, \mathcal Y^{DIB}_{N_d})$
   <!---->
   where $H(â‹…)$ is the entropy,
   $â„“_{CE}(f, \mathcal L)$ is the final cross-entropy loss when training $f$ on the dataset with labels $\mathcal L$,
   and $M_{N_d} = f_{N_d}^{L:l+1} âˆ˜ f^{l:1}$.

In addition, at the end of each epoch after the first, instead of copying $f^{l:1}$ and $f^{L:l+1}$ again,
simply update the parameters of $f^{l:1}$ with those of the original network, keeping $f^{l:1}$ frozen,
and reset the parameters of $\{f^{L:l+1}_{N_d}\}_{N_d = 1}^{N_D}$.
